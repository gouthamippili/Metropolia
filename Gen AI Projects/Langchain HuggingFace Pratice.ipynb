{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f90f718-c78a-4fce-abc6-81dfe515156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkipp\\AppData\\Local\\Temp\\ipykernel_7220\\3607173745.py:12: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "C:\\Users\\gkipp\\AppData\\Local\\Temp\\ipykernel_7220\\3607173745.py:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\gkipp\\AppData\\Local\\Temp\\ipykernel_7220\\3607173745.py:30: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm_chain.run(games=\"chess\")  # Pass a dictionary with the variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who won the chess championship in 2024?\n",
      "Let's think about this and answer.\n",
      "\n",
      "Answer: I cannot provide an answer to that question as the 2024 chess championship has not yet taken place. Chess championships are typically held annually, but the exact schedule and winners depend on the specific organization or league hosting the event. To find out who wins a particular chess championship, you would need to check the official website or news sources for updates after the event has taken place.\n"
     ]
    }
   ],
   "source": [
    "#Invoking model in hugging face using HuggingFaceHub\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token='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'\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"Question: who won the {games} championship in 2024?\n",
    "Let's think about this and answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"games\"],  # Fixed: input_variables instead of input_variable\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = llm_chain.run(games=\"chess\")  # Pass a dictionary with the variable\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844241f-2cfc-44aa-9325-3385376684d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking model in hugging face using CPU pipeline\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "model_id=\"meta-llama/Llama-3.2-1B\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
    "hf=HuggingFacePipeline(pipeline=pipe)\n",
    "hf.invoke(\"who is winner of world chess champion in 2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f81b16-d048-4843-8c2f-d73bba83cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking model in hugging face using GPU pipeline\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"meta-llama/Llama-3.2-1B\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain=prompt|gpu_llm\n",
    "question=\"What is artificial intelligence?\"\n",
    "response=chain.invoke({\"question\":question})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
